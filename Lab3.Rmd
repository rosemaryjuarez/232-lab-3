---
title: "Juarez_Lab3"
author: "Rosemary Juarez"
date: 
output: 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(glmnet)
```

## Lab 3: Predicting the age of abalone

Abalones are marine snails. Their flesh is widely considered to be a desirable food, and is consumed raw or cooked by a variety of cultures. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age.

The data set provided includes variables related to the sex, physical dimensions of the shell, and various weight measurements, along with the number of rings in the shell. Number of rings is the stand-in here for age.

### Data Exploration

Pull the abalone data from Github and take a look at it.

```{r data}
abdat<- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/abalone-data.csv")
glimpse(abdat)

```

### Data Splitting

-   ***Question 1***. Split the data into training and test sets. Use a 70/30 training/test split.

```{r}


set.seed(123) #set a seed for reproducibility
split <- initial_split(abdat)

#split is approx 70/30
split
data_train <-  training(split)
data_test  <- testing(split)
```


We'll follow our text book's lead and use the caret package in our approach to this task. We will use the glmnet package in order to perform ridge regression and the lasso. The main function in this package is glmnet(), which can be used to fit ridge regression models, lasso models, and more. In particular, we must pass in an x matrix of predictors as well as a y outcome vector , and we do not use the yâˆ¼x syntax.

### Fit a ridge regression model

-   ***Question 2***. Use the model.matrix() function to create a predictor matrix, x, and assign the Rings variable to an outcome vector, y.

```{r}
#Creating training feature matrices using model.matrix() (auto encoding of categorical variables)

X <- model.matrix(Rings ~ .,data_train)[,-1]

# transform y with log() transformation.]
Y <- log(data_train$Rings)

```

-   ***Question 3***. Fit a ridge model (controlled by the alpha parameter) using the glmnet() function. Make a plot showing how the estimated coefficients change with lambda. (Hint: You can call plot() directly on the glmnet() objects).

```{r}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0
)

#plotting the ridge model

plot(ridge, xvar = "lambda")
```


### Using *k*-fold cross validation resampling and tuning our models

In lecture we learned about two methods of estimating our model's generalization error by resampling, cross validation and bootstrapping. We'll use the *k*-fold cross validation method in this lab. Recall that lambda is a tuning parameter that helps keep our model from over-fitting to the training data. Tuning is the process of finding the optima value of lamba.


-   ***Question 4***. This time fit a ridge regression model and a lasso model, both with using cross validation. The glmnet package kindly provides a cv.glmnet() function to do this (similar to the glmnet() function that we just used). Use the alpha argument to control which type of model you are running. Plot the results.

```{r}
# Applying CV ridge regression to abdatdata
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Applying CV lasso regression to Abdat data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```


-   ***Question 5***. Interpret the graphs. What is being displayed on the axes here? How does the performance of the models change with the value of lambda?

The x axis for both graphs is displaying the range when using differing lambda variables. The left side of the x axis represents the smallest lambda unit we can use, while the right side shows a higher number. for the y-axis, we have the mean-squared error. We typically want to see a low error, so the lower mean-suqared error tells us that the lambda unit is good. since we see the lowest and most stable error between -4 and -2 for ridge penalty, that is likely the most optimal unit value to use(optimal lambda is represented by vertical dashes). For our Lasso, we see more gaps in our data(normal as we exclude some variables if insignificant the higher the lambda value), and see that our optimal lambda value has increased. We can choose between -8 and past -6 for our optimal lambda number. The performance of our model becomes less ideal, the higher the lambda value. Therefor it seems best to stick with lower lambda values according to this plot for the best model performance.



-   ***Question 6***. Inspect the ridge model object you created with cv.glmnet(). The \$cvm column shows the MSEs for each CV fold. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r}
#Ridge model
# minimum MSE
min(ridge$cvm) 

# lambda for this min MSE
ridge$lambda.min 

# 1-SE rule
ridge$cvm[ridge$lambda == ridge$lambda.1se] 

# lambda for this MSE
ridge$lambda.1se

```
For the ridge model, the minimum cvm column is 0.04229955. The value of the lambda associated with this minimum MSE is 0.02196803. If we want to do this using the 1-standard error rule(to prevent overfitting and giving us another outlook of otherwise unseen data), then the minimum MSE is 0.04349893 and the lambda value is 0.03497931. Overall the first yields better results than the second option, but not by much.

-   ***Question 7***. Do the same for the lasso model. What is the minimum MSE? What is the value of lambda associated with this MSE minimum?

```{r }

options(scipen = 100, digits = 4)

#Lasso model
min(lasso$cvm)       # minimum MSE

lasso$lambda.min     # lambda for this min MSE


# No. of coef | 1-SE MSE
lasso$nzero[lasso$lambda == lasso$lambda.min]
# 1-SE rule
lasso$lambda.1se  # lambda for this MSE

```
Similar to above, the minimum for lasso is 0.04013539. The lambda value for this would be  0.00009733 (i removed scientific notation). using 1-SE, lasso min is 10, and the lambda would be0.002772.

Data scientists often use the "one-standard-error" rule when tuning lambda to select the best model. This rule tells us to pick the most parsimonious model (fewest number of predictors) while still remaining within one standard error of the overall minimum cross validation error. The cv.glmnet() model object has a column that automatically finds the value of lambda associated with the model that produces an MSE that is one standard error from the MSE minimum (\$lambda.1se).

-   ***Question 8.*** Find the number of predictors associated with this model (hint: the \$nzero is the \# of predictors column).

```{r}

# least number of predictors associated within one standard error of the ridge model
ridge$nzero[ridge$lambda == ridge$lambda.1se]
# least number of predictors associated within one standard error of the lasso model
lasso$nzero[lasso$lambda == lasso$lambda.1se]
```
there are less number of predictors associated with lasso than with ridge(10 with ridge, 9 with lasso). This is because lasso will drop predictors that do not fit the data even with a very small lambda value. This is because it shows that it is likely insignificant or not influential to the model. 

-   ***Question 9*****.** Which regularized regression worked better for this task, ridge or lasso? Explain your answer.


I believe lasso did a better regularized regression, as both the penalty plot and minimum MSE values informed us which model did better. Lasso had more of the data within a lower MSE threshold, while ridge had a slightly higher consistent MSE the higher the lambda value. When looking at the minimum MSE values, we note that the minimum MSE values are much smaller than the ridge. Therefor I believe Lasso performed better than ridge.
